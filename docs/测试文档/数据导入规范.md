# 数据导入规范（Dataset / Import Job）

本文档定义“用户在平台内上传数据并导入到数据库/向量库”的统一规格，用作测试先行的验收基线。

## 目标
- 让用户在平台内完成：上传文件 → 预览与校验 → 选择目标数据源 → 受控导入 → 导入结果可观测与可回滚。
- 默认安全：查询接口保持只读；导入能力必须显式开启，并带资源限制与审计。

## 术语
- Dataset：用户上传的文件与其解析结果（采样、字段推断、校验结果），尚未写入任何数据源。
- Import Job：一次导入执行的可追踪对象（状态、统计、错误分布、可回滚信息）。
- Target：导入目标（SQL 表 / Neo4j label+relationship / Milvus collection+partition）。

## 功能要求
### Dataset
- 支持上传：CSV、JSON Lines（必需）；Parquet（可选）。
- 支持预览：展示采样行、推断字段类型、空值率、异常值样例。
- 支持校验：非法字段名、行长度超限、文件大小超限、编码错误等必须在导入前提示。

### Import Job
- 状态机：pending → running → succeeded / failed / canceled。
- 可取消：running 状态允许取消，取消后不再继续写入。
- 可追踪：必须输出写入统计（成功行数、失败行数、耗时、吞吐）与失败原因分布（按错误码聚合）。
- 可回滚：必须可追踪导入批次标识（import_batch_id），用于后续回滚删除。

## 数据源导入策略（必须支持）
### SQLite
- 目标：优先作为“离线一键可用”的默认导入目标。
- 建表：支持自动建表（字段推断）与追加写入（字段兼容时）。

### MySQL / PostgreSQL
- 只允许走导入 API 写入；不允许通过“SQL 查询接口”执行写操作。
- 写入策略：事务 + 批量写入 + 分片提交（防止单事务过大）。
- 行为必须可控：最大文件大小、最大行数、最大列数、最大字符串长度、超时、并发。

### Neo4j
- 写入必须参数化（禁止字符串拼接注入）。
- 支持两类导入：
  - 节点导入：label + props。
  - 关系导入：startId/endId + relationship type + props。
- 必须提供可回滚标识（batch 属性）以支持回滚清理。

### Milvus（社区版）
- 必须支持导入到指定 collection（及可选 partition）。
- 必须支持向量字段写入；向量维度不匹配时必须明确报错。
- 必须支持回滚策略（按主键/批次删除）。

## 安全与资源限制（验收必测）
- 默认只读：查询 API 永远不允许写入（即使导入能力开启）。
- 限额：文件大小、行数、列数、并发、超时、输出字节上限必须可配置且有默认值。
- 机密信息：token/password 不得出现在响应体与日志中（trace 中仅允许出现“已脱敏”的摘要）。
- SSRF 防护：对 Neo4j/Milvus baseUrl/uri 必须执行 scheme 与内网地址策略。

## Trace 与错误码（验收契约）
- Import Job 必须有 trace 概要字段（或可追溯日志 id），至少包含：
  - target 摘要（dsId、表/collection、schema/index）
  - 资源限制（哪些字段被 clamp）
  - 错误码与失败样例（采样）
- 建议错误码集合（最小）：missing_file、invalid_format、validation_failed、import_disabled、limit_exceeded、timeout、connect_failed、write_failed、rollback_failed。

## 测试落点建议（用于映射到 CI）
- PR 必跑（不依赖外部服务）：SQLite 导入端到端 + 负向用例（超大文件、非法字段名、写入被禁用）。
- Nightly：MySQL/Postgres/Neo4j/Milvus 全链路导入（容器化），覆盖回滚删除。

